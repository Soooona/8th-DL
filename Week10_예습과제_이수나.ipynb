{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 토큰화"
      ],
      "metadata": {
        "id": "TIAVNaI8cvek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review = \"현실과 구분 불가능한 cg. 시각적 즐거움은 최고!\"\n",
        "tokenized = review.split()\n",
        "print(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcLD_EYu77__",
        "outputId": "89d1f175-f6e2-474e-f890-bd05465b762b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['현실과', '구분', '불가능한', 'cg.', '시각적', '즐거움은', '최고!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review = \"현실과 구분 불가능한 cg. 시각적 즐거움은 최고!\"\n",
        "tokenized =list(review)\n",
        "print(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8coZ8yIP8Ipu",
        "outputId": "b6974a73-27ed-4f7f-ae57-2bfbe1131d38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['현', '실', '과', ' ', '구', '분', ' ', '불', '가', '능', '한', ' ', 'c', 'g', '.', ' ', '시', '각', '적', ' ', '즐', '거', '움', '은', ' ', '최', '고', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jamo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6MwubZw8Wmn",
        "outputId": "344d89c9-4b56-4cd6-ca8d-e7d42ddc79e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jamo\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: jamo\n",
            "Successfully installed jamo-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jamo import h2j, j2hcj\n",
        "\n",
        "review = \"현실과 구분 불가능한 cg. 시각적 즐거움은 최고!\"\n",
        "decomposed = j2hcj(h2j(review))\n",
        "tokenized = list(decomposed)\n",
        "print(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdwNXFZ88NSf",
        "outputId": "b71833cc-bbfb-49ec-c413-1038aef15be4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ㅎ', 'ㅕ', 'ㄴ', 'ㅅ', 'ㅣ', 'ㄹ', 'ㄱ', 'ㅘ', ' ', 'ㄱ', 'ㅜ', 'ㅂ', 'ㅜ', 'ㄴ', ' ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㄱ', 'ㅏ', 'ㄴ', 'ㅡ', 'ㅇ', 'ㅎ', 'ㅏ', 'ㄴ', ' ', 'c', 'g', '.', ' ', 'ㅅ', 'ㅣ', 'ㄱ', 'ㅏ', 'ㄱ', 'ㅈ', 'ㅓ', 'ㄱ', ' ', 'ㅈ', 'ㅡ', 'ㄹ', 'ㄱ', 'ㅓ', 'ㅇ', 'ㅜ', 'ㅁ', 'ㅇ', 'ㅡ', 'ㄴ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Okt 토큰화"
      ],
      "metadata": {
        "id": "a2Vxv2vXctqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmOx9wXOdYyf",
        "outputId": "5781014f-724c-4450-c52f-756e5b535f7f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.2 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "sentence = \"무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.\"\n",
        "\n",
        "nouns = okt.nouns(sentence)\n",
        "phrases = okt.phrases(sentence)\n",
        "pos = okt.pos(sentence)\n",
        "morphs = okt.morphs(sentence)\n",
        "\n",
        "print(\"명사 추출: \",nouns)\n",
        "print(\"구 추출: \", phrases)\n",
        "print(\"형태소 추출: \", morphs)\n",
        "print(\"품사 태깅: \", pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCXCN6gtctWx",
        "outputId": "aab25f85-e6ac-4224-d3f4-a49313fa4e1d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "명사 추출:  ['무엇', '상상', '수', '사람', '무엇', '낼', '수']\n",
            "구 추출:  ['무엇', '상상', '상상할 수', '상상할 수 있는 사람', '사람']\n",
            "형태소 추출:  ['무엇', '이든', '상상', '할', '수', '있는', '사람', '은', '무엇', '이든', '만들어', '낼', '수', '있다', '.']\n",
            "품사 태깅:  [('무엇', 'Noun'), ('이든', 'Josa'), ('상상', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있는', 'Adjective'), ('사람', 'Noun'), ('은', 'Josa'), ('무엇', 'Noun'), ('이든', 'Josa'), ('만들어', 'Verb'), ('낼', 'Noun'), ('수', 'Noun'), ('있다', 'Adjective'), ('.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "\n",
        "sentence = \"무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.\"\n",
        "\n",
        "nouns = kkma.nouns(sentence)\n",
        "phrases = kkma.sentences(sentence)\n",
        "pos = kkma.pos(sentence)\n",
        "morphs = kkma.morphs(sentence)\n",
        "\n",
        "print(\"명사 추출: \",nouns)\n",
        "print(\"구 추출: \", phrases)\n",
        "print(\"형태소 추출: \", morphs)\n",
        "print(\"품사 태깅: \", pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1BIseredVWO",
        "outputId": "13d5b3d7-0f9f-483a-a25d-0f99aaaafcce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "명사 추출:  ['무엇', '상상', '수', '사람', '무엇']\n",
            "구 추출:  ['무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.']\n",
            "형태소 추출:  ['무엇', '이', '든', '상상', '하', 'ㄹ', '수', '있', '는', '사람', '은', '무엇', '이', '든', '만들', '어', '내', 'ㄹ', '수', '있', '다', '.']\n",
            "품사 태깅:  [('무엇', 'NNG'), ('이', 'VCP'), ('든', 'ECE'), ('상상', 'NNG'), ('하', 'XSV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('는', 'ETD'), ('사람', 'NNG'), ('은', 'JX'), ('무엇', 'NP'), ('이', 'VCP'), ('든', 'ECE'), ('만들', 'VV'), ('어', 'ECD'), ('내', 'VXV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('다', 'EFN'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK"
      ],
      "metadata": {
        "id": "X5-mV6YfeZYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmOxUyq4eW18",
        "outputId": "2b987f4f-ef88-4b54-8f6b-2bc82b5a4e7c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk6AWjANekcA",
        "outputId": "f7da3fc1-61d2-4a30-bd87-cd06aa41f486"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tokenize\n",
        "\n",
        "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
        "\n",
        "word_tokens = tokenize.word_tokenize(sentence)\n",
        "sent_tokens = tokenize.sent_tokenize(sentence)\n",
        "\n",
        "print(word_tokens)\n",
        "print(sent_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsD7r7yJeoN2",
        "outputId": "0e63eb32-9943-49d8-b919-dfa0fd750982"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Those', 'who', 'can', 'imagine', 'anything', ',', 'can', 'create', 'the', 'impossible', '.']\n",
            "['Those who can imagine anything, can create the impossible.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tag\n",
        "from nltk import tokenize\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
        "\n",
        "word_tokens = tokenize.word_tokenize(sentence)\n",
        "pos = tag.pos_tag(word_tokens)\n",
        "\n",
        "print(pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9SXg_UYe1f1",
        "outputId": "0da2e8f6-68d0-4815-c209-8a1e1bb1cfb2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Those', 'DT'), ('who', 'WP'), ('can', 'MD'), ('imagine', 'VB'), ('anything', 'NN'), (',', ','), ('can', 'MD'), ('create', 'VB'), ('the', 'DT'), ('impossible', 'JJ'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## spaCy"
      ],
      "metadata": {
        "id": "MB-O8IYsfO5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spcay download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COO_mGZ9fLb1",
        "outputId": "bb62afc4-bdaf-4679-9be8-e3e8fe8800bf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "/usr/bin/python3: No module named spcay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
        "\n",
        "doc = nlp(sentence)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"[{token.pos_:5} - {token.tag_:3}] : {token.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZdcMW0nfWPn",
        "outputId": "122e2a2b-4705-41ab-f79e-e379a8f942c6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PRON  - DT ] : Those\n",
            "[PRON  - WP ] : who\n",
            "[AUX   - MD ] : can\n",
            "[VERB  - VB ] : imagine\n",
            "[PRON  - NN ] : anything\n",
            "[PUNCT - ,  ] : ,\n",
            "[AUX   - MD ] : can\n",
            "[VERB  - VB ] : create\n",
            "[DET   - DT ] : the\n",
            "[ADJ   - JJ ] : impossible\n",
            "[PUNCT - .  ] : .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 센텐스피스"
      ],
      "metadata": {
        "id": "FSOC17ZNfkpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece Korpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqePN7yXfl2P",
        "outputId": "d7e1f983-b937-4678-c9c4-2e8b0e0fa736"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: Korpora in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.11/dist-packages (from Korpora) (0.6)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (4.67.1)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.32.3)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Korpora import Korpora\n",
        "\n",
        "corpus = Korpora.load(\"korean_petitions\")\n",
        "dataset = corpus.train\n",
        "petition = dataset[0]\n",
        "\n",
        "print(\"청원 시작일:\", petition.begin)\n",
        "print(\"청원 종료일:\", petition.end)\n",
        "print(\"청원 동의수:\", petition.num_agree)\n",
        "print(\"청원 범주:\", petition.category)\n",
        "print(\"청원 제목:\", petition.title)\n",
        "print(\"청원 본문:\", petition.text[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnhC3lJEfqV-",
        "outputId": "30d88452-3b7b-4f46-d535-9d6be69dcf11"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : Hyunjoong Kim lovit@github\n",
            "    Repository : https://github.com/lovit/petitions_archive\n",
            "    References :\n",
            "\n",
            "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
            "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
            "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
            "    단 청원의 동의 개수는 수집됩니다.\n",
            "    자세한 내용은 위의 repository를 참고하세요.\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[korean_petitions] download petitions_2017-08: 1.84MB [00:00, 15.2MB/s]                           \n",
            "[korean_petitions] download petitions_2017-09: 20.4MB [00:00, 54.4MB/s]                            \n",
            "[korean_petitions] download petitions_2017-10: 12.0MB [00:00, 31.0MB/s]                            \n",
            "[korean_petitions] download petitions_2017-11: 28.4MB [00:00, 40.2MB/s]                            \n",
            "[korean_petitions] download petitions_2017-12: 29.0MB [00:00, 33.2MB/s]                            \n",
            "[korean_petitions] download petitions_2018-01: 43.9MB [00:00, 79.9MB/s]                            \n",
            "[korean_petitions] download petitions_2018-02: 33.8MB [00:00, 62.6MB/s]                            \n",
            "[korean_petitions] download petitions_2018-03: 34.3MB [00:00, 163MB/s]                            \n",
            "[korean_petitions] download petitions_2018-04: 35.5MB [00:00, 149MB/s]                            \n",
            "[korean_petitions] download petitions_2018-05: 37.5MB [00:00, 60.2MB/s]                            \n",
            "[korean_petitions] download petitions_2018-06: 37.8MB [00:00, 81.4MB/s]                            \n",
            "[korean_petitions] download petitions_2018-07: 40.5MB [00:00, 70.0MB/s]                            \n",
            "[korean_petitions] download petitions_2018-08: 39.8MB [00:00, 163MB/s]                            \n",
            "[korean_petitions] download petitions_2018-09: 36.1MB [00:00, 82.9MB/s]                            \n",
            "[korean_petitions] download petitions_2018-10: 38.1MB [00:00, 64.0MB/s]                            \n",
            "[korean_petitions] download petitions_2018-11: 37.7MB [00:00, 89.2MB/s]                           \n",
            "[korean_petitions] download petitions_2018-12: 33.0MB [00:00, 71.5MB/s]                            \n",
            "[korean_petitions] download petitions_2019-01: 34.8MB [00:00, 143MB/s]                            \n",
            "[korean_petitions] download petitions_2019-02: 30.8MB [00:00, 73.2MB/s]                            \n",
            "[korean_petitions] download petitions_2019-03: 34.9MB [00:00, 106MB/s]                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "청원 시작일: 2017-08-25\n",
            "청원 종료일: 2017-09-24\n",
            "청원 동의수: 88\n",
            "청원 범주: 육아/교육\n",
            "청원 제목: 학교는 인력센터, 취업센터가 아닙니다. 정말 간곡히 부탁드립니다.\n",
            "청원 본문: 안녕하세요. 현재 사대, 교대 등 교원양성학교들의 예비\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Korpora import Korpora\n",
        "\n",
        "corpus = Korpora.load(\"korean_petitions\")\n",
        "petitions = corpus.get_all_texts()\n",
        "with open(\"petition.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for petition in petitions:\n",
        "        f.write(petition+\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74s-K3C0wUIV",
        "outputId": "323cd257-3325-4383-fcb7-af33e814f3ab"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : Hyunjoong Kim lovit@github\n",
            "    Repository : https://github.com/lovit/petitions_archive\n",
            "    References :\n",
            "\n",
            "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
            "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
            "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
            "    단 청원의 동의 개수는 수집됩니다.\n",
            "    자세한 내용은 위의 repository를 참고하세요.\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-08\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-09\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-10\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-11\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-12\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-01\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-02\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-03\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-04\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-05\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-06\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-07\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-08\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-09\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-10\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-11\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-12\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2019-01\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2019-02\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2019-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentencepiece import SentencePieceTrainer\n",
        "\n",
        "SentencePieceTrainer.Train(\n",
        "    \"--input=/content/petition.txt --model_prefix=petition_bpe --vocab_size=8000 model_type=bpe\"\n",
        ")"
      ],
      "metadata": {
        "id": "eIBJDs1kgOKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentencepiece import SentencePieceProcessor\n",
        "\n",
        "tokenizer = SentencePieceProcessor()\n",
        "tokenizer.load(\"petition_bpe.model\")\n",
        "\n",
        "sentence=\"안녕하세요, 토크나이저가 잘 학습되었군요!\"\n",
        "sentences = [\"이렇게 입력값을 리스트로 받아서\", \"쉽게 토크나이저를 사용할 수 있답니다\"]\n",
        "\n",
        "tokenized_sentence = tokenizer.encode_as_pieces(sentence)\n",
        "tokenized_sentences = tokenizer.encode_as_pieces(sentences)\n",
        "\n",
        "print(\"단일 문장 토큰화 :\",tokenized_sentence)\n",
        "print(\"여러 문장 토큰화 :\",tokenized_sentences)\n",
        "\n",
        "encoded_sentence = tokenizer.encode_as_ids(sentence)\n",
        "encoded_sentences = tokenizer.encode_as_ids(sentences)\n",
        "\n",
        "print(\"단일 문장 인코딩 :\",encoded_sentence)\n",
        "print(\"여러 문장 인코딩 :\",encoded_sentences)\n",
        "\n",
        "decode_ids = tokenizer.decode_ids(encoded_sentences)\n",
        "decode_pieces = tokenizer.decode_pieces(tokenized_sentences)\n",
        "\n",
        "print(\"인코딩 값 디코딩 :\",decode_ids)\n",
        "print(\"인코딩 값 디코딩 :\",decode_pieces)"
      ],
      "metadata": {
        "id": "tLDzBYR0xNwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentencepiece import SentencePieceTrainer\n",
        "\n",
        "tokenizer = SentencePieceProcessor()\n",
        "tokenizer.load(\"petition_bpe.model\")\n",
        "\n",
        "vocab = {idx: tokenizer.id_to_piece(idx) for idx in range(tokenizer.get_piece_size())}\n",
        "print(list(vocab.items())[:5])\n",
        "print(\"vocab size :\", len(vocab))"
      ],
      "metadata": {
        "id": "DY0edu9x6n1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 토크나이저스"
      ],
      "metadata": {
        "id": "mnSlC6NQysSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_wh08-cyt10",
        "outputId": "7cf97f5c-e883-4189-c155-612c634e62f3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.normalizers import NFD, Lowercase\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
      ],
      "metadata": {
        "id": "S0_CtGavywjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. 임베딩"
      ],
      "metadata": {
        "id": "56EB8yUf27oX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-gram"
      ],
      "metadata": {
        "id": "MI23rc2T2_W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def ngrams(sentence, n):\n",
        "    words = sentence.split()\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "    return list(ngrams)\n",
        "\n",
        "sentence=\"안녕하세요 만나서 진심으로 반가워요\"\n",
        "\n",
        "unigram = ngrams(sentence,1)\n",
        "bigram = ngrams(sentence,2)\n",
        "trigram = ngrams(sentence,3)\n",
        "\n",
        "print(unigram)\n",
        "print(bigram)\n",
        "print(trigram)\n",
        "\n",
        "unigram = nltk.ngrams(sentence.split(),1)\n",
        "bigram = nltk.ngrams(sentence.split(),2)\n",
        "trigram = nltk.ngrams(sentence.split(),3)\n",
        "\n",
        "print(list(unigram))\n",
        "print(list(bigram))\n",
        "print(list(trigram))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hMnkeG029Fi",
        "outputId": "3417d79b-95c5-4a3b-a576-32e327a31e02"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]\n",
            "[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]\n",
            "[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]\n",
            "[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]\n",
            "[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]\n",
            "[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF"
      ],
      "metadata": {
        "id": "zMRWEJtR3ntI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCDTRfJw3s4K",
        "outputId": "967d9108-35a7-4a5d-a3fb-9f4c60ea0022"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    input=\"content\",\n",
        "    encoding=\"utf-8\",\n",
        "    lowercase=True,\n",
        "    stop_words=\"None\",\n",
        "    ngram_range=(1,1),\n",
        "    max_df=1.0,\n",
        "    min_df=1,\n",
        "    vocabulary=None,\n",
        "    smooth_idf=True,\n",
        ")"
      ],
      "metadata": {
        "id": "Hc-DmiVk3fZX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"That movie is famous movie\",\n",
        "    \"I like that actor\",\n",
        "    \"I don't like that actor\"\n",
        "]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer.fit(corpus)\n",
        "tfidf_matrix = tfidf_vectorizer.transform(corpus)\n",
        "\n",
        "print(tfidf_matrix.toarray())\n",
        "print(tfidf_vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lQLxUvh4GQm",
        "outputId": "1c0b4a5f-0dbe-429d-aa56-ee0b2bbe5bda"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.39687454 0.39687454 0.         0.79374908\n",
            "  0.2344005 ]\n",
            " [0.61980538 0.         0.         0.         0.61980538 0.\n",
            "  0.48133417]\n",
            " [0.4804584  0.63174505 0.         0.         0.4804584  0.\n",
            "  0.37311881]]\n",
            "{'that': 6, 'movie': 5, 'is': 3, 'famous': 2, 'like': 4, 'actor': 0, 'don': 1}\n"
          ]
        }
      ]
    }
  ]
}